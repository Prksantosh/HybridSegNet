{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d15665a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                        Vision Transformer (ViT) Summary                        \n",
      "================================================================================\n",
      "Image size: 224x224\n",
      "Patch size: 16x16\n",
      "Input channels: 3\n",
      "Embedding dimension: 768\n",
      "Number of heads: 8\n",
      "Number of layers: 8\n",
      "MLP ratio: 4.0\n",
      "Number of classes: 2\n",
      "================================================================================\n",
      "\n",
      "Layer Details:\n",
      "\n",
      "Input              | Shape: (batch_size, 3, 224, 224) | Params: 0         | Details: Input image\n",
      "PatchEmbedding     | Shape: (batch_size, 196, 768)    | Params: 590,592   | Details: img_size=224, patch_size=16, in_channels=3, embed_dim=768\n",
      "PositionEmbedding  | Shape: (1, 197, 768)             | Params: 152,064   | Details: Learned positional embeddings + class token\n",
      "PosDrop            | Shape: (batch_size, 197, 768)    | Params: 0         | Details: Dropout(p=0.1)\n",
      "TransformerBlock_1 | Shape: (batch_size, 197, 768)    | Params: 7,087,872 | Details: dim=768, num_heads=8, mlp_ratio=4.0\n",
      "TransformerBlock_2 | Shape: (batch_size, 197, 768)    | Params: 7,087,872 | Details: dim=768, num_heads=8, mlp_ratio=4.0\n",
      "TransformerBlock_3 | Shape: (batch_size, 197, 768)    | Params: 7,087,872 | Details: dim=768, num_heads=8, mlp_ratio=4.0\n",
      "TransformerBlock_4 | Shape: (batch_size, 197, 768)    | Params: 7,087,872 | Details: dim=768, num_heads=8, mlp_ratio=4.0\n",
      "TransformerBlock_5 | Shape: (batch_size, 197, 768)    | Params: 7,087,872 | Details: dim=768, num_heads=8, mlp_ratio=4.0\n",
      "TransformerBlock_6 | Shape: (batch_size, 197, 768)    | Params: 7,087,872 | Details: dim=768, num_heads=8, mlp_ratio=4.0\n",
      "TransformerBlock_7 | Shape: (batch_size, 197, 768)    | Params: 7,087,872 | Details: dim=768, num_heads=8, mlp_ratio=4.0\n",
      "TransformerBlock_8 | Shape: (batch_size, 197, 768)    | Params: 7,087,872 | Details: dim=768, num_heads=8, mlp_ratio=4.0\n",
      "LayerNorm          | Shape: (batch_size, 197, 768)    | Params: 1,536     | Details: LayerNorm(768)\n",
      "Head               | Shape: (batch_size, 2)           | Params: 1,538     | Details: Linear(768->2)\n",
      "\n",
      "================================================================================\n",
      "Total Parameters   : 57,448,706\n",
      "================================================================================\n",
      "\n",
      "Standard Torch Summary:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "    PatchEmbedding-2             [-1, 196, 768]               0\n",
      "           Dropout-3             [-1, 197, 768]               0\n",
      "         LayerNorm-4             [-1, 197, 768]           1,536\n",
      "            Linear-5            [-1, 197, 2304]       1,771,776\n",
      "           Dropout-6          [-1, 8, 197, 197]               0\n",
      "            Linear-7             [-1, 197, 768]         590,592\n",
      "           Dropout-8             [-1, 197, 768]               0\n",
      "         Attention-9             [-1, 197, 768]               0\n",
      "        LayerNorm-10             [-1, 197, 768]           1,536\n",
      "           Linear-11            [-1, 197, 3072]       2,362,368\n",
      "             GELU-12            [-1, 197, 3072]               0\n",
      "          Dropout-13            [-1, 197, 3072]               0\n",
      "           Linear-14             [-1, 197, 768]       2,360,064\n",
      "          Dropout-15             [-1, 197, 768]               0\n",
      "              MLP-16             [-1, 197, 768]               0\n",
      " TransformerBlock-17             [-1, 197, 768]               0\n",
      "        LayerNorm-18             [-1, 197, 768]           1,536\n",
      "           Linear-19            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-20          [-1, 8, 197, 197]               0\n",
      "           Linear-21             [-1, 197, 768]         590,592\n",
      "          Dropout-22             [-1, 197, 768]               0\n",
      "        Attention-23             [-1, 197, 768]               0\n",
      "        LayerNorm-24             [-1, 197, 768]           1,536\n",
      "           Linear-25            [-1, 197, 3072]       2,362,368\n",
      "             GELU-26            [-1, 197, 3072]               0\n",
      "          Dropout-27            [-1, 197, 3072]               0\n",
      "           Linear-28             [-1, 197, 768]       2,360,064\n",
      "          Dropout-29             [-1, 197, 768]               0\n",
      "              MLP-30             [-1, 197, 768]               0\n",
      " TransformerBlock-31             [-1, 197, 768]               0\n",
      "        LayerNorm-32             [-1, 197, 768]           1,536\n",
      "           Linear-33            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-34          [-1, 8, 197, 197]               0\n",
      "           Linear-35             [-1, 197, 768]         590,592\n",
      "          Dropout-36             [-1, 197, 768]               0\n",
      "        Attention-37             [-1, 197, 768]               0\n",
      "        LayerNorm-38             [-1, 197, 768]           1,536\n",
      "           Linear-39            [-1, 197, 3072]       2,362,368\n",
      "             GELU-40            [-1, 197, 3072]               0\n",
      "          Dropout-41            [-1, 197, 3072]               0\n",
      "           Linear-42             [-1, 197, 768]       2,360,064\n",
      "          Dropout-43             [-1, 197, 768]               0\n",
      "              MLP-44             [-1, 197, 768]               0\n",
      " TransformerBlock-45             [-1, 197, 768]               0\n",
      "        LayerNorm-46             [-1, 197, 768]           1,536\n",
      "           Linear-47            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-48          [-1, 8, 197, 197]               0\n",
      "           Linear-49             [-1, 197, 768]         590,592\n",
      "          Dropout-50             [-1, 197, 768]               0\n",
      "        Attention-51             [-1, 197, 768]               0\n",
      "        LayerNorm-52             [-1, 197, 768]           1,536\n",
      "           Linear-53            [-1, 197, 3072]       2,362,368\n",
      "             GELU-54            [-1, 197, 3072]               0\n",
      "          Dropout-55            [-1, 197, 3072]               0\n",
      "           Linear-56             [-1, 197, 768]       2,360,064\n",
      "          Dropout-57             [-1, 197, 768]               0\n",
      "              MLP-58             [-1, 197, 768]               0\n",
      " TransformerBlock-59             [-1, 197, 768]               0\n",
      "        LayerNorm-60             [-1, 197, 768]           1,536\n",
      "           Linear-61            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-62          [-1, 8, 197, 197]               0\n",
      "           Linear-63             [-1, 197, 768]         590,592\n",
      "          Dropout-64             [-1, 197, 768]               0\n",
      "        Attention-65             [-1, 197, 768]               0\n",
      "        LayerNorm-66             [-1, 197, 768]           1,536\n",
      "           Linear-67            [-1, 197, 3072]       2,362,368\n",
      "             GELU-68            [-1, 197, 3072]               0\n",
      "          Dropout-69            [-1, 197, 3072]               0\n",
      "           Linear-70             [-1, 197, 768]       2,360,064\n",
      "          Dropout-71             [-1, 197, 768]               0\n",
      "              MLP-72             [-1, 197, 768]               0\n",
      " TransformerBlock-73             [-1, 197, 768]               0\n",
      "        LayerNorm-74             [-1, 197, 768]           1,536\n",
      "           Linear-75            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-76          [-1, 8, 197, 197]               0\n",
      "           Linear-77             [-1, 197, 768]         590,592\n",
      "          Dropout-78             [-1, 197, 768]               0\n",
      "        Attention-79             [-1, 197, 768]               0\n",
      "        LayerNorm-80             [-1, 197, 768]           1,536\n",
      "           Linear-81            [-1, 197, 3072]       2,362,368\n",
      "             GELU-82            [-1, 197, 3072]               0\n",
      "          Dropout-83            [-1, 197, 3072]               0\n",
      "           Linear-84             [-1, 197, 768]       2,360,064\n",
      "          Dropout-85             [-1, 197, 768]               0\n",
      "              MLP-86             [-1, 197, 768]               0\n",
      " TransformerBlock-87             [-1, 197, 768]               0\n",
      "        LayerNorm-88             [-1, 197, 768]           1,536\n",
      "           Linear-89            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-90          [-1, 8, 197, 197]               0\n",
      "           Linear-91             [-1, 197, 768]         590,592\n",
      "          Dropout-92             [-1, 197, 768]               0\n",
      "        Attention-93             [-1, 197, 768]               0\n",
      "        LayerNorm-94             [-1, 197, 768]           1,536\n",
      "           Linear-95            [-1, 197, 3072]       2,362,368\n",
      "             GELU-96            [-1, 197, 3072]               0\n",
      "          Dropout-97            [-1, 197, 3072]               0\n",
      "           Linear-98             [-1, 197, 768]       2,360,064\n",
      "          Dropout-99             [-1, 197, 768]               0\n",
      "             MLP-100             [-1, 197, 768]               0\n",
      "TransformerBlock-101             [-1, 197, 768]               0\n",
      "       LayerNorm-102             [-1, 197, 768]           1,536\n",
      "          Linear-103            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-104          [-1, 8, 197, 197]               0\n",
      "          Linear-105             [-1, 197, 768]         590,592\n",
      "         Dropout-106             [-1, 197, 768]               0\n",
      "       Attention-107             [-1, 197, 768]               0\n",
      "       LayerNorm-108             [-1, 197, 768]           1,536\n",
      "          Linear-109            [-1, 197, 3072]       2,362,368\n",
      "            GELU-110            [-1, 197, 3072]               0\n",
      "         Dropout-111            [-1, 197, 3072]               0\n",
      "          Linear-112             [-1, 197, 768]       2,360,064\n",
      "         Dropout-113             [-1, 197, 768]               0\n",
      "             MLP-114             [-1, 197, 768]               0\n",
      "TransformerBlock-115             [-1, 197, 768]               0\n",
      "       LayerNorm-116             [-1, 197, 768]           1,536\n",
      "          Linear-117                    [-1, 2]           1,538\n",
      "================================================================\n",
      "Total params: 57,296,642\n",
      "Trainable params: 57,296,642\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 245.18\n",
      "Params size (MB): 218.57\n",
      "Estimated Total Size (MB): 464.32\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import sqrt\n",
    "from collections import OrderedDict\n",
    "from torchsummary import summary\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Split image into patches and project them into embedding space.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input shape: (batch_size, channels, height, width)\n",
    "        Output shape: (batch_size, n_patches, embed_dim)\n",
    "        \"\"\"\n",
    "        x = self.proj(x)  # (batch_size, embed_dim, n_patches_h, n_patches_w)\n",
    "        x = x.flatten(2)  # (batch_size, embed_dim, n_patches)\n",
    "        x = x.transpose(1, 2)  # (batch_size, n_patches, embed_dim)\n",
    "        return x\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return (f\"img_size={self.img_size}, patch_size={self.patch_size}, \"\n",
    "                f\"in_channels={self.proj.in_channels}, embed_dim={self.proj.out_channels}\")\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # each of shape (B, num_heads, N, head_dim)\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        \n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return f\"dim={self.dim}, num_heads={self.num_heads}, head_dim={self.head_dim}\"\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multilayer Perceptron with GELU activation and dropout.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features * 4\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return (f\"in_features={self.fc1.in_features}, \"\n",
    "                f\"hidden_features={self.fc1.out_features}, \"\n",
    "                f\"out_features={self.fc2.out_features}\")\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer block with attention and residual connections.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(\n",
    "            in_features=dim,\n",
    "            hidden_features=int(dim * mlp_ratio),\n",
    "            drop=drop\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return (f\"dim={self.attn.dim}, num_heads={self.attn.num_heads}, \"\n",
    "                f\"mlp_ratio={self.mlp.fc1.out_features/self.attn.dim:.1f}\")\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard Vision Transformer (ViT) implementation.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=224,\n",
    "        patch_size=16,\n",
    "        in_channels=3,\n",
    "        num_classes=1000,\n",
    "        embed_dim=768,\n",
    "        depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.,\n",
    "        qkv_bias=True,\n",
    "        drop_rate=0.,\n",
    "        attn_drop_rate=0.,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = depth\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_channels=in_channels,\n",
    "            embed_dim=embed_dim\n",
    "        )\n",
    "        \n",
    "        # Class token and position embedding\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.patch_embed.n_patches + 1, embed_dim))\n",
    "        \n",
    "        # Dropout after positional embedding\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "            )\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        # Head\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)  # (B, n_patches, embed_dim)\n",
    "        \n",
    "        # Add class token\n",
    "        cls_token = self.cls_token.expand(B, -1, -1)  # (B, 1, embed_dim)\n",
    "        x = torch.cat((cls_token, x), dim=1)  # (B, n_patches+1, embed_dim)\n",
    "        \n",
    "        # Add position embedding\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Classification head\n",
    "        x = self.norm(x)\n",
    "        cls_token_final = x[:, 0]  # Take only the class token\n",
    "        x = self.head(cls_token_final)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_layer_specs(self):\n",
    "        \"\"\"Returns a detailed summary of each layer's specifications\"\"\"\n",
    "        specs = OrderedDict()\n",
    "        \n",
    "        # Input specs\n",
    "        specs[\"Input\"] = {\n",
    "            \"shape\": f\"(batch_size, {self.in_channels}, {self.img_size}, {self.img_size})\",\n",
    "            \"params\": 0,\n",
    "            \"details\": \"Input image\"\n",
    "        }\n",
    "        \n",
    "        # Patch embedding\n",
    "        patch_params = sum(p.numel() for p in self.patch_embed.parameters())\n",
    "        specs[\"PatchEmbedding\"] = {\n",
    "            \"shape\": f\"(batch_size, {self.patch_embed.n_patches}, {self.embed_dim})\",\n",
    "            \"params\": patch_params,\n",
    "            \"details\": self.patch_embed.extra_repr()\n",
    "        }\n",
    "        \n",
    "        # Class token and position embedding\n",
    "        pos_params = self.pos_embed.numel() + self.cls_token.numel()\n",
    "        specs[\"PositionEmbedding\"] = {\n",
    "            \"shape\": f\"(1, {self.patch_embed.n_patches + 1}, {self.embed_dim})\",\n",
    "            \"params\": pos_params,\n",
    "            \"details\": \"Learned positional embeddings + class token\"\n",
    "        }\n",
    "        \n",
    "        # Positional dropout\n",
    "        specs[\"PosDrop\"] = {\n",
    "            \"shape\": f\"(batch_size, {self.patch_embed.n_patches + 1}, {self.embed_dim})\",\n",
    "            \"params\": 0,\n",
    "            \"details\": f\"Dropout(p={self.pos_drop.p})\"\n",
    "        }\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            block_params = sum(p.numel() for p in block.parameters())\n",
    "            specs[f\"TransformerBlock_{i+1}\"] = {\n",
    "                \"shape\": f\"(batch_size, {self.patch_embed.n_patches + 1}, {self.embed_dim})\",\n",
    "                \"params\": block_params,\n",
    "                \"details\": block.extra_repr()\n",
    "            }\n",
    "        \n",
    "        # Final normalization\n",
    "        norm_params = sum(p.numel() for p in self.norm.parameters())\n",
    "        specs[\"LayerNorm\"] = {\n",
    "            \"shape\": f\"(batch_size, {self.patch_embed.n_patches + 1}, {self.embed_dim})\",\n",
    "            \"params\": norm_params,\n",
    "            \"details\": f\"LayerNorm({self.embed_dim})\"\n",
    "        }\n",
    "        \n",
    "        # Classification head\n",
    "        head_params = sum(p.numel() for p in self.head.parameters()) if self.num_classes > 0 else 0\n",
    "        specs[\"Head\"] = {\n",
    "            \"shape\": f\"(batch_size, {self.num_classes})\",\n",
    "            \"params\": head_params,\n",
    "            \"details\": f\"Linear({self.embed_dim}->{self.num_classes})\" if self.num_classes > 0 else \"Identity\"\n",
    "        }\n",
    "        \n",
    "        return specs\n",
    "\n",
    "def print_model_summary(model):\n",
    "    \"\"\"Prints a detailed summary of the model architecture\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Vision Transformer (ViT) Summary':^80}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Image size: {model.img_size}x{model.img_size}\")\n",
    "    print(f\"Patch size: {model.patch_size}x{model.patch_size}\")\n",
    "    print(f\"Input channels: {model.in_channels}\")\n",
    "    print(f\"Embedding dimension: {model.embed_dim}\")\n",
    "    print(f\"Number of heads: {model.num_heads}\")\n",
    "    print(f\"Number of layers: {model.depth}\")\n",
    "    print(f\"MLP ratio: {model.blocks[0].mlp.fc1.out_features/model.embed_dim:.1f}\")\n",
    "    print(f\"Number of classes: {model.num_classes}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nLayer Details:\\n\")\n",
    "    \n",
    "    specs = model.get_layer_specs()\n",
    "    max_layer_len = max(len(name) for name in specs.keys())\n",
    "    max_shape_len = max(len(spec[\"shape\"]) for spec in specs.values())\n",
    "    max_params_len = max(len(f\"{spec['params']:,}\") if isinstance(spec[\"params\"], int) \n",
    "                        else len(spec[\"params\"]) for spec in specs.values())\n",
    "    \n",
    "    total_params = 0\n",
    "    \n",
    "    for name, spec in specs.items():\n",
    "        params = spec[\"params\"]\n",
    "        if isinstance(params, int):\n",
    "            total_params += params\n",
    "            params_str = f\"{params:,}\"\n",
    "        else:\n",
    "            params_str = params\n",
    "            \n",
    "        print(f\"{name:<{max_layer_len}} | Shape: {spec['shape']:<{max_shape_len}} | \"\n",
    "              f\"Params: {params_str:<{max_params_len}} | Details: {spec['details']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"{'Total Parameters':<{max_layer_len}} : {total_params:,}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Create and test the model\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration (same as original)\n",
    "    config = {\n",
    "        \"img_size\": 224,\n",
    "        \"patch_size\": 16,\n",
    "        \"in_channels\": 3,\n",
    "        \"num_classes\": 2,\n",
    "        \"embed_dim\": 768,\n",
    "        \"depth\": 8,\n",
    "        \"num_heads\": 8,\n",
    "        \"mlp_ratio\": 4.0,\n",
    "        \"qkv_bias\": True,\n",
    "        \"drop_rate\": 0.1,\n",
    "        \"attn_drop_rate\": 0.1\n",
    "    }\n",
    "    \n",
    "    # Create model\n",
    "    model = VisionTransformer(**config)\n",
    "    \n",
    "    # Print detailed summary\n",
    "    print_model_summary(model)\n",
    "    \n",
    "    # Print standard torchsummary\n",
    "    print(\"\\nStandard Torch Summary:\")\n",
    "    summary(model, (3, 224, 224), device=\"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (tensorflow-gpu)",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
